# Lead Harvest Pipeline

Automated workflow that discovers businesses from Google Maps, crawls their digital surface area with **Crawl4AI**, enriches contact data with **GPT-4o-mini**, follows important external profiles, and persists findings to Supabase. The pipeline intentionally avoids CSS selectors and regex-based scraping—every extraction step relies on LLM reasoning and prompt-crafted extraction schemas.

## Features
- Google Maps semantic extraction via trimmed `APP_INITIALIZATION_STATE` payloads processed by GPT-4o-mini.
- Intelligent link scoring to prioritize contact/team/leadership pages.
- Person-level contact parsing (names, roles, emails, phones, socials) entirely through LLM extraction—no regex/selectors.
- External network enrichment (LinkedIn, Twitter/X, YouTube, etc.).
- Wayback Machine snapshot traversal for every crawled internal/external URL (basic validation done; see limitations).
- Native Supabase upserts for business and contact tables with deduplication and field-level enrichment.

## Requirements
- Python 3.10+
- Access to the internet (blocked in some testing environments).
- API keys:
  - `OPENAI_API_KEY` (GPT-4o or compatible model).
  - `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY` (or `SUPABASE_ANON_KEY` for testing).

Install dependencies:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Environment Variables

```bash
export OPENAI_API_KEY="sk-..."
export OPENAI_MODEL="openai/gpt-4o"            # optional (defaults to gpt-4o)
export OPENAI_TEMPERATURE="0.0"                # optional
export OPENAI_MAX_TOKENS="1800"                # optional

export SUPABASE_URL="https://xyzcompany.supabase.co"
export SUPABASE_SERVICE_ROLE_KEY="service-role-key"
export SUPABASE_BUSINESS_TABLE="businesses"    # optional
export SUPABASE_CONTACT_TABLE="contacts"       # optional

export CRAWL_MAX_INTERNAL_LINKS="15"           # optional tuning
export CRAWL_MAX_EXTERNAL_LINKS="10"
export CRAWL_LINK_CONCURRENCY="5"
export WAYBACK_SNAPSHOT_LIMIT="5"
export WAYBACK_YEARS_BACK="5"
export CRAWL_USE_CACHE="false"
```

## Running the Pipeline

```bash
python3 main.py "lawyers in New York, NY" --output leads.json
```

The command prints the structured contact map to stdout and (optionally) stores it to a JSON file. Results are also upserted into Supabase when the database credentials are provided. Use `--max-businesses N` to limit processing during validation, e.g. `--max-businesses 1` for a single firm trial.

### Backfilling from JSON exports

If you already have a `leads*.json` file and want to push it into Supabase manually, use the helper script:

```bash
python import_to_supabase.py --input leads_single.json --default-query "lawyers in New York, NY"
```

The script reads the JSON structure generated by `main.py`, creates minimal business rows (using the provided default query), and upserts all contact records.

## Supabase Schema Expectations

Create two tables (adjust names via env variables if needed):

```sql
create table businesses (
  id uuid primary key default gen_random_uuid(),
  name text,
  query text,
  address text,
  phone text,
  website text,
  google_maps_url text,
  rating numeric,
  review_count int,
  additional_metadata jsonb,
  inserted_at timestamptz default now(),
  updated_at timestamptz default now()
);

create table contacts (
  id uuid primary key default gen_random_uuid(),
  business_name text,
  person_name text,
  position text,
  emails text[],
  phone_numbers text[],
  social_links text[],
  location text,
  notes text,
  source_url text,
  source_type text,
  snapshot_timestamp text,
  inserted_at timestamptz default now(),
  updated_at timestamptz default now()
);

create unique index contacts_dedupe_key
  on contacts (business_name, person_name, position, source_type, snapshot_timestamp);
```

## Wayback Machine Behavior
- For every live URL we crawl (homepage, prioritized internal links, and high-signal external profiles), the pipeline also visits `https://web.archive.org/web/*/<URL>` and uses GPT-4o-mini to select the best historical snapshots within the configured time window.
- Each snapshot is crawled with the same person-level extraction strategy used on the live site.
- Contacts obtained from archives are flagged with `source_type = "wayback"` and carry a `snapshot_timestamp`.
- **Current status:** Basic snapshot selection is functional but not stress-tested across a wide range of sites. Expect occasional misses or low-confidence results until further tuning.

## Notes & Limitations
- The pipeline relies on GPT-driven extraction only—no CSS selectors or regex are used for data parsing.
- Runs have been validated end-to-end on a single firm (Google Maps → site + socials → Supabase). Scale cautiously and monitor OpenAI rate limits.
- Wayback-derived data is experimental; verify archival contacts manually before consuming them in production workflows.
- Heavy Wayback usage can slow runs considerably—keep `WAYBACK_SNAPSHOT_LIMIT` tuned appropriately for your rate/usage budget.
- Google Maps content can change dynamically. Adjust the model temperature or instructions if extractions become noisy.
- Respect target-site Terms of Service and rate limits. Crawl4AI's adaptive link scorer already limits excess crawling, but you can further tune `max_links` and concurrency.
- Costs: LLM extraction happens multiple times per business (homepage, contact pages, external profiles, snapshots). Monitor usage and consider batching queries.
- Deduplication merges newly discovered emails/phones/socials into existing contacts, so repeated runs enrich existing rows instead of creating duplicates.
